# ğŸ§  AdaBoost Algorithm from Scratch

> Dive into the inner workings of one of the most powerful ensemble learning techniques â€“ built step-by-step, from the ground up!

---

## ğŸ¯ Objective

To implement the **AdaBoost Classifier** algorithm manually using basic machine learning and data manipulation libraries to gain deep intuition of how it really worksâ€”beyond the black-box of `scikit-learn`.

### ğŸ§¾ What is AdaBoost?

**AdaBoost** (short for *Adaptive Boosting*) is an ensemble learning algorithm that combines multiple weak learners (typically simple decision stumps) into a single strong classifier. It works by assigning weights to training examples, emphasizing those that were previously misclassified, and giving more influence to better-performing models through calculated importance weights (alphas).

---

## ğŸ” Scope

This project emphasizes **understanding the logic** behind AdaBoost rather than optimizing for performance or accuracy.

---

## âš™ï¸ Algorithm Breakdown

1. **Data Preparation**
2. **Weight Initialization** â€“ Uniform weights for all samples
3. **Train Weak Learners** â€“ Using decision stumps (`max_depth = 1`)
4. **Make Predictions**
5. **Calculate Alpha** â€“ The importance of each model
6. **Update Sample Weights** â€“ Focus more on misclassified samples
7. **Resample Data** â€“ Based on updated weights
8. **Repeat** â€“ Re-train for `n` models (e.g. 2 in this example)
